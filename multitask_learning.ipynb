{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f769aa4",
   "metadata": {},
   "source": [
    "# Multi-Task Learning (MTL)\n",
    "Task is to take a student's data as input and predict:\n",
    "1.\tRegression: The student's final grade (G3), a number from 0 to 20.\n",
    "2.\tClassification: Whether the student is in a romantic relationship (romantic), a \"yes\" or \"no\" value.\n",
    "\n",
    "The hypothesis is that a shared \"student profile\" (learned by the network's body) can help predict both academic performance and personal life.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5598c6",
   "metadata": {},
   "source": [
    "# 1. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bbea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Data\n",
    "file_path = \"./data/student/student-mat.csv\"\n",
    "\n",
    "data = pd.read_csv(file_path, sep=';')\n",
    "print('Loaded:', file_path)\n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1846c7",
   "metadata": {},
   "source": [
    "### 1.1 Define Targets\n",
    "We will use:\n",
    "- Regression target: G3 (final grade, 0-20)\n",
    "- Classification target: romantic (\"yes\"/\"no\") transformed to binary 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ae18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression target\n",
    "data['y_grade'] = data['G3'].astype(int)\n",
    "\n",
    "# classification target: map yes/no to 1/0\n",
    "data['y_romantic'] = data['romantic'].map({'yes': 1, 'no': 0}).astype(int)\n",
    "\n",
    "print(data[['G3', 'y_grade', 'romantic', 'y_romantic']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf88c6a",
   "metadata": {},
   "source": [
    "    ### 1.2 Quick Checks on Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b33f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grade distribution\n",
    "print('Grade (G3) distribution:')\n",
    "print(data['y_grade'].describe())\n",
    "\n",
    "# romantic balance\n",
    "print('\\nRomantic class balance:')\n",
    "print(data['y_romantic'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a277a210",
   "metadata": {},
   "source": [
    "## 2. Handle Categorical Features\n",
    "Convert non-numeric columns into numeric form using one-hot encoding. Binary columns (yes/no) are mapped to 1 and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Separate categorical and numeric columns\n",
    "cat_cols = [c for c in data.columns if data[c].dtype == 'object' and c not in ['romantic']]\n",
    "num_cols = [c for c in data.columns if data[c].dtype != 'object' and c not in ['y_grade', 'y_romantic']]\n",
    "\n",
    "print('Categorical columns:', cat_cols)\n",
    "print('Numeric columns:', num_cols)\n",
    "\n",
    "# Define preprocessing pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), num_cols),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "])\n",
    "\n",
    "# Fit and transform data\n",
    "X = preprocessor.fit_transform(data)\n",
    "y_grade = data['y_grade'].values.astype(np.int8)\n",
    "y_romantic = data['y_romantic'].values.astype(np.int8)\n",
    "\n",
    "print('Processed feature shape:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e47724c",
   "metadata": {},
   "source": [
    "## 3. Split Data\n",
    "Splitting the data ensures the model is trained on one subset (train), tuned on another unseen subset (validation), and finally evaluated on a completely unseen subset (test) to prevent overfitting and to estimate generalization performance.\n",
    "\n",
    "We split into train, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_grade_train, y_grade_temp, y_romantic_train, y_romantic_temp = train_test_split(\n",
    "    X, y_grade, y_romantic, test_size=0.3, random_state=42)\n",
    "\n",
    "X_val, X_test, y_grade_val, y_grade_test, y_romantic_val, y_romantic_test = train_test_split(\n",
    "    X_temp, y_grade_temp, y_romantic_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f'Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43910fa",
   "metadata": {},
   "source": [
    "## 4. PyTorch Dataset Class\n",
    "Create a custom dataset returning (x, y_grade, y_romantic) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StudentDataset(Dataset):\n",
    "    def __init__(self, X, y_grade, y_romantic):\n",
    "        self.X = torch.tensor(X.todense() if hasattr(X, 'todense') else X, dtype=torch.float32)\n",
    "        self.y_grade = torch.tensor(y_grade, dtype=torch.int8).unsqueeze(1)\n",
    "        self.y_romantic = torch.tensor(y_romantic, dtype=torch.int8)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y_grade[idx], self.y_romantic[idx]\n",
    "\n",
    "# Create datasets\n",
    "dset_train = StudentDataset(X_train, y_grade_train, y_romantic_train)\n",
    "dset_val = StudentDataset(X_val, y_grade_val, y_romantic_val)\n",
    "dset_test = StudentDataset(X_test, y_grade_test, y_romantic_test)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(dset_train, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dset_val, batch_size=32)\n",
    "test_loader = DataLoader(dset_test, batch_size=32)\n",
    "\n",
    "print('DataLoaders ready!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c34fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify one batch.\n",
    "for xb, yb_grade, yb_romantic in train_loader:\n",
    "    print('Batch X:', xb.shape)\n",
    "    print('Batch y_grade:', yb_grade.shape)\n",
    "    print('Batch y_romantic:', yb_romantic.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd4b1b7",
   "metadata": {},
   "source": [
    "## 5. Building the Multi-Head Model\n",
    "Define a neural network that shares a common body and branches into two separate heads: one for regression (predicting grades) and one for classification (predicting romantic status).\n",
    "\n",
    "The shared body learns a general representation of the student data. Two heads specialize for their respective tasks:\n",
    "- Regression head outputs a single scalar (predicted grade).\n",
    "- Classification head outputs two values (for romantic status yes/no)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26716b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, input_dim, shared_dim=128, dropout_p=0.3):\n",
    "        super(MultiTaskModel, self).__init__()\n",
    "        \n",
    "        # Shared body\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_dim, shared_dim),\n",
    "            nn.BatchNorm1d(shared_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(shared_dim, shared_dim // 2),\n",
    "            nn.BatchNorm1d(shared_dim // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Regression head (Grade prediction)\n",
    "        self.head_regression = nn.Sequential(\n",
    "            nn.Linear(shared_dim // 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        # Classification head (Romantic status)\n",
    "        self.head_classification = nn.Sequential(\n",
    "            nn.Linear(shared_dim // 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)  # two values for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_features = self.shared(x)\n",
    "        out_grade = self.head_regression(shared_features)\n",
    "        out_romantic = self.head_classification(shared_features)\n",
    "        return out_grade, out_romantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ebb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialization\n",
    "input_dim = X_train.shape[1]\n",
    "model = MultiTaskModel(input_dim=input_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a69c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass Check\n",
    "\n",
    "xb, yb_grade, yb_romantic = next(iter(train_loader))\n",
    "\n",
    "out_grade, out_romantic = model(xb)\n",
    "print('Grade output shape:', out_grade.shape)\n",
    "print('Romantic output shape:', out_romantic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86055d23",
   "metadata": {},
   "source": [
    "## 6. Custom Training Loop\n",
    "In this section, we train our multi-task network using two different loss functions — one for regression (MSELoss) and one for classification (CrossEntropyLoss). We combine them into a single total loss for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cb1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Define Losses and Optimizer\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define loss functions\n",
    "criterion_regression = nn.MSELoss()          # for grade prediction\n",
    "criterion_classification = nn.CrossEntropyLoss()  # for romantic status\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "def train_epoch(model, loader, optimizer, criterion_reg, criterion_cls, device):\n",
    "    model.train()\n",
    "    total_loss, total_reg, total_cls = 0, 0, 0\n",
    "\n",
    "    for xb, yb_grade, yb_romantic in loader:\n",
    "        xb, yb_grade, yb_romantic = xb.to(device), yb_grade.to(device), yb_romantic.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        out_grade, out_romantic = model(xb)\n",
    "\n",
    "        # Compute individual losses\n",
    "        loss_reg = criterion_reg(out_grade.squeeze(), yb_grade.float())\n",
    "        loss_cls = criterion_cls(out_romantic, yb_romantic.long())\n",
    "\n",
    "        # Combine losses: 2 approaches\n",
    "        # loss = loss_reg + loss_cls\n",
    "        loss = 0.8 * loss_reg + (1 - 0.8) * loss_cls\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_reg += loss_reg.item()\n",
    "        total_cls += loss_cls.item()\n",
    "\n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_reg/n, total_cls/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3790a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Loop\n",
    "\n",
    "def validate_epoch(model, loader, criterion_reg, criterion_cls, device):\n",
    "    model.eval()\n",
    "    total_loss, total_reg, total_cls = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb_grade, yb_romantic in loader:\n",
    "            xb, yb_grade, yb_romantic = xb.to(device), yb_grade.to(device), yb_romantic.to(device)\n",
    "            \n",
    "            out_grade, out_romantic = model(xb)\n",
    "            \n",
    "            loss_reg = criterion_reg(out_grade.squeeze(), yb_grade.float())\n",
    "            loss_cls = criterion_cls(out_romantic, yb_romantic.long())\n",
    "            \n",
    "            loss = loss_reg + loss_cls\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_reg += loss_reg.item()\n",
    "            total_cls += loss_cls.item()\n",
    "\n",
    "    n = len(loader)\n",
    "    return total_loss/n, total_reg/n, total_cls/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9d953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Validate Over Epochs\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_reg_losses, val_reg_losses = [], []\n",
    "train_cls_losses, val_cls_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    tr_loss, tr_reg, tr_cls = train_epoch(model, train_loader, optimizer, criterion_regression, criterion_classification, device)\n",
    "    val_loss, val_reg, val_cls = validate_epoch(model, val_loader, criterion_regression, criterion_classification, device)\n",
    "\n",
    "    train_losses.append(tr_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_reg_losses.append(tr_reg)\n",
    "    val_reg_losses.append(val_reg)\n",
    "    train_cls_losses.append(tr_cls)\n",
    "    val_cls_losses.append(val_cls)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Total Loss: {tr_loss:.4f} | Val Loss: {val_loss:.4f} | Reg: {val_reg:.4f} | Cls: {val_cls:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67283568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ploting (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Plot 1: Total Loss ---\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(train_losses, label='Train Total Loss', color='blue')\n",
    "plt.plot(val_losses, label='Val Total Loss', color='orange')\n",
    "plt.title('Total Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# --- Plot 2: Grade (Regression) Loss ---\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(train_reg_losses, label='Train Grade Loss', color='green')\n",
    "plt.plot(val_reg_losses, label='Val Grade Loss', color='red')\n",
    "plt.title('Grade (Regression) Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# --- Plot 3: Romantic (Classification) Loss ---\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(train_cls_losses, label='Train Romantic Loss', color='purple')\n",
    "plt.plot(val_cls_losses, label='Val Romantic Loss', color='brown')\n",
    "plt.title('Romantic (Classification) Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1560709",
   "metadata": {},
   "source": [
    "## 7. Evaluation & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b99ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, f1_score\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    # Forward pass on test set\n",
    "    x_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_grade_pred, y_romantic_pred = model(x_test_tensor)\n",
    "\n",
    "    # --- Grade Prediction (Regression) ---\n",
    "    # Convert predictions and targets to numpy\n",
    "    y_grade_pred_np = y_grade_pred.squeeze().numpy()\n",
    "    y_grade_true_np = y_grade_test\n",
    "\n",
    "    # Calculate Mean Absolute Error\n",
    "    mae = mean_absolute_error(y_grade_true_np, y_grade_pred_np)\n",
    "\n",
    "    # --- Romantic Status (Classification) ---\n",
    "    # Get predicted class (0 or 1)\n",
    "    y_romantic_pred_class = torch.argmax(y_romantic_pred, dim=1).numpy()\n",
    "    y_romantic_true_np = y_romantic_test\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    accuracy = accuracy_score(y_romantic_true_np, y_romantic_pred_class)\n",
    "\n",
    "    # Calculate F1-Score for 'yes' class (1 = 'yes')\n",
    "    f1 = f1_score(y_romantic_true_np, y_romantic_pred_class, pos_label=1)\n",
    "\n",
    "# --- Final Report ---\n",
    "print(\"=== Test Set Evaluation ===\")\n",
    "\n",
    "print(\"-\"*50)\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data.numpy().flatten())\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(f\"Grade Prediction (MAE): {mae:.4f}\")\n",
    "print(f\"Romantic Status (Accuracy): {accuracy:.4f}\")\n",
    "print(f\"Romantic Status (F1-Score): {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c919537b",
   "metadata": {},
   "source": [
    "## 8. Results For First Pass\n",
    "\n",
    "```\n",
    "=== Test Set Evaluation ===\n",
    "Grade Prediction (MAE): 3.7891\n",
    "Romantic Status (Accuracy): 0.5333\n",
    "Romantic Status (F1-Score): 0.3333\n",
    "```\n",
    "\n",
    "Grade prediction is off by 3.8 score on average which is 20% of maximum score. The model correctly predicts romantic status for only about half of the students, which is barely better than random guessing. The F1-score of 0.33 tells us it’s performing poorly on the minority class (\"yes\"), missing many true positives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3190ea51",
   "metadata": {},
   "source": [
    "## 9. How to Improve?\n",
    "\n",
    "The main idea is to give different “importance” to each task when computing the total loss. Right now, you’re doing: `loss = loss_reg + loss_cls`.\n",
    "\n",
    "Here, loss_reg (MSE) might be 25 and loss_cls (CrossEntropy) might be 0.6, so the network focuses mostly on grade prediction. To fix this, you can introduce a weighting hyperparameter alpha (between 0.0 and 1.0) and compute: `loss = alpha * loss_reg + (1 - alpha) * loss_cls`.\n",
    "\n",
    "Table of how it affected MAE, Accuracy and F1:\n",
    "| Alpha | MAE    | Accuracy | F1     |\n",
    "| ----- | ------ | -------- | ------ |\n",
    "| 0.2   | 3.8320 | 0.5667   | 0.3500 |\n",
    "| 0.5   | 3.6765 | 0.5667   | 0.3158 |\n",
    "| 0.8   | 3.7729 | 0.4833   | 0.2439 |\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- Alpha = 0.2 → more weight on romantic status (classification):\n",
    "    - F1 is highest (0.3500), MAE is slightly worse.\n",
    "    - Classification improves because the network “cares more” about this task.\n",
    "\n",
    "- Alpha = 0.5 → equal weighting:\n",
    "    - MAE is slightly better (3.6765) than 0.2, F1 slightly lower (0.3158).\n",
    "    - Balanced performance between regression and classification.\n",
    "\n",
    "- Alpha = 0.8 → more weight on grade prediction (regression):\n",
    "    - MAE is comparable but F1 drops significantly (0.2439).\n",
    "    - The network prioritizes reducing regression error at the expense of classification performance.\n",
    "\n",
    "Conclusion:\n",
    "\n",
    "Weighted loss lets you control which task the model prioritizes, and you can tune alpha depending on whether you care more about grades or romantic status. This is exactly why multi-task learning often requires careful loss balancing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
